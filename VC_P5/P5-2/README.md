# P5-2 - Filtros de emoci√≥n con modelo propio

## Objetivo de la pr√°ctica

Desarrollar **un prototipo** de tem√°tica libre que provoquen
reacciones o efectos a partir de la informaci√≥n extra√≠da del rostro. Uno
de los prototipos debe incluir el uso de un **modelo entrenado por
nosotros** para la extracci√≥n de informaci√≥n biom√©trica (en este caso,
clasificaci√≥n de emociones faciales).

Este repositorio incluye:

1.  **Prototipo 1 (Climas emocionales)**: Usa nuestro
    modelo ResNet50 entrenado para clasificar 6 emociones y reacciona
    con fondos animados.
2.  **Prototipo 2 (Monos lado a lado)**: Compara la detecci√≥n y
    clasificaci√≥n de nuestro modelo con una visualizaci√≥n de "monos
    emocionales" en tiempo real, mostrando webcam y mono
    simult√°neamente.

------------------------------------------------------------------------

## Qu√© hay en esta carpeta

    P5/
    ‚îú‚îÄ‚îÄ VC_P5_emotion_detector.ipynb                  # Prototipo 1: fondos + efectos faciales
    ‚îú‚îÄ‚îÄ VC_P5_emotion_detector_monkeys.ipynb          # Prototipo 2: monos lado a lado
    ‚îú‚îÄ‚îÄ best_model_emotions.pth                       # Modelo ResNet50 entrenado (6 emociones)
    ‚îî‚îÄ‚îÄ README.md

------------------------------------------------------------------------

## Requisitos

Se recomienda usar un entorno conda. Instalaci√≥n de dependencias:

    conda create -n VC_P5_emotions python=3.11
    conda activate VC_P5_emotions

    # OpenCV + PyTorch
    pip install opencv-python
    pip install torch torchvision torchaudio
    pip install numpy matplotlib

    # DeepFace (detecci√≥n facial)
    pip install deepface
    pip install tf-keras  # necesario para TensorFlow 2.20+
------------------------------------------------------------------------

## Dataset Utilizado

Hemos utilizado este dataset:

https://drive.google.com/file/d/1PhX7oIvhZqzj74YIjbRxfXCpUKGnaEre/view?usp=sharing

------------------------------------------------------------------------

## üìä Resultados del entrenamiento del modelo

El modelo ResNet50 fue entrenado para la clasificaci√≥n de **6 emociones faciales**
(angry, fear, happy, neutral, sad, surprise), utilizando *fine-tuning parcial* sobre
pesos preentrenados en ImageNet.

A continuaci√≥n se muestra la gr√°fica de p√©rdida (*loss*) y precisi√≥n (*accuracy*)
para entrenamiento y validaci√≥n:

![Resultados Entrenamiento](../images/resultado-entrenamiento.png)

---

### üß† An√°lisis t√©cnico de las curvas

#### **1. Curvas de p√©rdida (Loss)**  
En la gr√°fica izquierda se observa:

- **Train Loss**: desciende progresivamente desde ~1.75 hasta valores cercanos a 0.9,
  indicando una reducci√≥n sostenida del error durante la optimizaci√≥n.
- **Validation Loss**: disminuye inicialmente (1.35 ‚Üí ~1.1), pero se estabiliza entre
  1.1 y 1.15, con fluctuaciones peque√±as en √©pocas posteriores.

**Interpretaci√≥n t√©cnica:**

- El modelo muestra **convergencia s√≥lida**, sin oscilaciones abruptas.
- La validaci√≥n entra en un r√©gimen **estacionario** tras ~10 √©pocas.
- Aparece un **overfitting leve** a partir de la √©poca 15: el train loss contin√∫a
  bajando mientras que el val loss deja de mejorar, comportamiento habitual en
  clasificadores de emociones por la alta variabilidad del gesto facial.

---

#### **2. Curvas de precisi√≥n (Accuracy)**  
En la gr√°fica derecha:

- **Train Accuracy** progresa desde ~0.35 hasta ~0.78, evidencia del aprendizaje
  efectivo en las capas superiores finamente ajustadas.
- **Validation Accuracy** alcanza r√°pidamente ~0.67 en las primeras √©pocas y se
  mantiene estable entre 0.66 y 0.69 durante el resto del entrenamiento.

**Interpretaci√≥n:**

- El modelo presenta una **generalizaci√≥n aceptable**, sin ca√≠das bruscas en val_acc.
- El ‚Äúgap‚Äù de ~10‚Äì12% entre entrenamiento y validaci√≥n indica un **sobreajuste leve**,
  pero dentro del rango normal para datasets de expresiones faciales sin
  augmentations agresivos.
- La estabilidad constante de la validaci√≥n sugiere que el modelo ha alcanzado su
  **capacidad representativa m√°xima** con la configuraci√≥n actual.

----------------------------------

## Prototipo 1 --- Climas emocionales + efectos faciales

### Descripci√≥n

-   **Prop√≥sito**: Demostrar un sistema completo de reacci√≥n emocional
    usando un modelo de deep learning propio (ResNet50) para clasificar
    6 emociones (angry, fear, happy, neutral, sad, surprise) y generar
    efectos visuales en tiempo real.
-   **Comportamiento**:
    -   Detecta la cara con DeepFace (backend opencv).
    -   Clasifica la emoci√≥n con el modelo ResNet50 entrenado.
    -   Dibuja un **fondo animado** seg√∫n la emoci√≥n (sol para happy,
        lluvia para sad, llamas para angry, etc.).
    -   Marca la cara con **HUD en esquinas**.

### Fragmento de c√≥digo (bucle principal resumido)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)
        orig_frame = frame.copy()
        label = "neutral"
        face_box = None

        # 1. Detectar cara con DeepFace
        try:
            detections = DeepFace.extract_faces(
                img_path=orig_frame,
                detector_backend="opencv",
                enforce_detection=False
            )
            if len(detections) > 0:
                det = detections
                fa = det["facial_area"]
                x, y, w, h = fa["x"], fa["y"], fa["w"], fa["h"]
                face_box = (x, y, w, h)
                face_bgr = orig_frame[y:y+h, x:x+w]
                label, probs = predict_emotion(face_bgr)
        except Exception:
            label = "neutral"

        # 2. Pintar fondo seg√∫n emoci√≥n
        draw_emotion_background(frame, label)

        # 3. Restaurar cara + HUD + efectos
        if face_box is not None:
            x, y, w, h = face_box
            face_roi = orig_frame[y:y+h, x:x+w]
            frame[y:y+h, x:x+w] = face_roi
            draw_face_hud_and_halo(frame, x, y, w, h)
            draw_face_effects(frame, x, y, w, h, label)

        cv2.imshow("Emotion Backgrounds + Effects", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

## Gif Demo de clima por emoci√≥n


![Clima - anotado 1](./demo-clima.gif)

------------------------------------------------------------------------

## Prototipo 2 --- Monos lado a lado (comparativa)

### Descripci√≥n

-   **Prop√≥sito**: Comparar visualmente la detecci√≥n y clasificaci√≥n de
    nuestro modelo ResNet50 con una representaci√≥n simb√≥lica (im√°genes
    de "monos" seg√∫n emoci√≥n). Permite evaluar la precisi√≥n del modelo
    de forma intuitiva.
-   **Comportamiento**:
    -   Lado izquierdo: webcam con rect√°ngulo verde alrededor de la cara
        detectada + texto con emoci√≥n y FPS.
    -   Lado derecho: imagen del mono correspondiente a la emoci√≥n
        detectada (con suavizado temporal para evitar cambios bruscos).

### Fragmento de c√≥digo (bucle principal resumido)

    historial = deque(maxlen=5)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)
        orig_frame = frame.copy()
        annotated_frame = frame.copy()
        label = "neutral"

        # 1. Detectar cara y clasificar emoci√≥n
        try:
            detections = DeepFace.extract_faces(
                img_path=orig_frame,
                detector_backend="opencv",
                enforce_detection=False
            )
            if len(detections) > 0:
                det = detections
                fa = det["facial_area"]
                x, y, w, h = fa["x"], fa["y"], fa["w"], fa["h"]
                face_bgr = orig_frame[y:y+h, x:x+w]
                label, probs = predict_emotion(face_bgr)
                cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        except Exception:
            label = "neutral"

        # 2. Suavizado temporal
        historial.append(label)
        label_suavizado = Counter(historial).most_common(1)

        # 3. Obtener mono y combinar lado a lado
        mono_key = emotion_to_mono.get(label_suavizado, "neutral")
        mono_img = monos.get(mono_key, monos.get('neutral'))
        mono_resized = redimensionar_imagen(mono_img, annotated_frame.shape)

        if mono_resized is not None:
            combined = np.hstack((annotated_frame, mono_resized))
        else:
            combined = annotated_frame

        cv2.imshow('Espejo con el mono (Modelo propio)', combined)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

### Mapeo de emociones a monos

    emotion_to_mono = {
        "angry": "enfadado",
        "fear": "sorprendido",
        "happy": "feliz",
        "neutral": "neutral",
        "sad": "pensando",
        "surprise": "sorprendido"
    }

## Gif Demo de monos

![Monos - anotado 1](./demo-mono.gif)

------------------------------------------------------------------------

## Detalles del modelo

### Arquitectura: ResNet50 + cabecera personalizada

    base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
    base.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(2048, 512),
        nn.ReLU(),
        nn.BatchNorm1d(512),
        nn.Dropout(0.4),
        nn.Linear(512, 6)
    )

### Preprocesamiento de entrada

-   Tama√±o: 224√ó224
-   Escala de grises ‚Üí 3 canales
-   Normalizaci√≥n ImageNet

------------------------------------------------------------------------

## Comparaci√≥n con MediaPipe

El enfoque propuesto usa: - **DeepFace** para detecci√≥n facial. -
**ResNet50** para clasificaci√≥n end-to-end.

MediaPipe usa landmarks + reglas heur√≠sticas para deducir emociones.

------------------------------------------------------------------------

## Cr√©ditos

-   ResNet50 (PyTorch)
-   DeepFace backend OpenCV
-   Im√°genes de monos (recursos libres)
-   Desarrollado para la asignatura de Visi√≥n por Computador

------------------------------------------------------------------------
